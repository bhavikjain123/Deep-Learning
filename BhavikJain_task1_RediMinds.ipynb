{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdb47cb",
   "metadata": {},
   "source": [
    "## Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e6a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bhavi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv3D, MaxPooling3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5f6c6",
   "metadata": {},
   "source": [
    "## Define the path to the dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9214c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"Real Life Violence Dataset\"\n",
    "\n",
    "# Define the paths to the \"Violence\" and \"NonViolence\" subfolders\n",
    "violence_dir = os.path.join(dataset_dir, \"Violence\")\n",
    "non_violence_dir = os.path.join(dataset_dir, \"NonViolence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57243c",
   "metadata": {},
   "source": [
    "## Define constants and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab632bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and configuration\n",
    "IMG_SIZE = (224, 224)\n",
    "NUM_FRAMES = 16\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694d738",
   "metadata": {},
   "source": [
    "## Function to extract video frames and preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed97dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract video frames and preprocess them\n",
    "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)\n",
    "    frames = []\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, IMG_SIZE)\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308ffff",
   "metadata": {},
   "source": [
    "## Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7bfd840",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for label, label_dir in [(\"Violence\", violence_dir), (\"NonViolence\", non_violence_dir)]:\n",
    "    for video_filename in os.listdir(label_dir):\n",
    "        video_path = os.path.join(label_dir, video_filename)\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) == NUM_FRAMES:\n",
    "            X.append(frames)\n",
    "            y.append(1 if label == \"Violence\" else 0)\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a21d1",
   "metadata": {},
   "source": [
    "## Split data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d15ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d98e5d",
   "metadata": {},
   "source": [
    "## Define the 3D CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cededb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bhavi\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(NUM_FRAMES,) + IMG_SIZE + (3,)),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n",
    "    MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32e69f",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f16b7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\bhavi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bhavi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "45/45 [==============================] - 1118s 25s/step - loss: 259.3086 - accuracy: 0.5864 - val_loss: 0.6135 - val_accuracy: 0.7063\n",
      "Epoch 2/15\n",
      "45/45 [==============================] - 1030s 23s/step - loss: 0.7200 - accuracy: 0.6894 - val_loss: 0.6303 - val_accuracy: 0.6750\n",
      "Epoch 3/15\n",
      "45/45 [==============================] - 1010s 23s/step - loss: 0.5700 - accuracy: 0.6720 - val_loss: 0.6162 - val_accuracy: 0.6313\n",
      "Epoch 4/15\n",
      "45/45 [==============================] - 963s 21s/step - loss: 0.5299 - accuracy: 0.7082 - val_loss: 0.6789 - val_accuracy: 0.7063\n",
      "Epoch 5/15\n",
      "45/45 [==============================] - 971s 22s/step - loss: 0.4595 - accuracy: 0.7688 - val_loss: 0.6567 - val_accuracy: 0.7250\n",
      "Epoch 6/15\n",
      "45/45 [==============================] - 926s 21s/step - loss: 0.5159 - accuracy: 0.7792 - val_loss: 0.6848 - val_accuracy: 0.6875\n",
      "Epoch 7/15\n",
      "45/45 [==============================] - 962s 21s/step - loss: 0.3861 - accuracy: 0.8182 - val_loss: 0.8028 - val_accuracy: 0.7250\n",
      "Epoch 8/15\n",
      "45/45 [==============================] - 999s 22s/step - loss: 0.3405 - accuracy: 0.8405 - val_loss: 0.7627 - val_accuracy: 0.7063\n",
      "Epoch 9/15\n",
      "45/45 [==============================] - 961s 21s/step - loss: 0.3142 - accuracy: 0.8482 - val_loss: 1.0556 - val_accuracy: 0.7250\n",
      "Epoch 10/15\n",
      "45/45 [==============================] - 905s 20s/step - loss: 0.3204 - accuracy: 0.8600 - val_loss: 0.7986 - val_accuracy: 0.7375\n",
      "Epoch 11/15\n",
      "45/45 [==============================] - 908s 20s/step - loss: 0.2783 - accuracy: 0.8733 - val_loss: 0.8705 - val_accuracy: 0.7875\n",
      "Epoch 12/15\n",
      "45/45 [==============================] - 938s 21s/step - loss: 0.2456 - accuracy: 0.8948 - val_loss: 0.8156 - val_accuracy: 0.8125\n",
      "Epoch 13/15\n",
      "45/45 [==============================] - 910s 20s/step - loss: 0.2221 - accuracy: 0.9088 - val_loss: 0.7825 - val_accuracy: 0.8000\n",
      "Epoch 14/15\n",
      "45/45 [==============================] - 932s 21s/step - loss: 0.2644 - accuracy: 0.8942 - val_loss: 0.8189 - val_accuracy: 0.8000\n",
      "Epoch 15/15\n",
      "45/45 [==============================] - 916s 20s/step - loss: 0.2138 - accuracy: 0.9143 - val_loss: 0.8595 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eb0913c990>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614311d",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f186fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 44s 2s/step - loss: 1.0837 - accuracy: 0.7925\n",
      "Test Loss: 1.0837340354919434, Test Accuracy: 0.7925000190734863\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394aaa52",
   "metadata": {},
   "source": [
    "## Extract and display frames from a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e43977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames(video_path, num_frames=5):\n",
    "    frames = extract_frames(video_path, num_frames)\n",
    "    for i, frame in enumerate(frames):\n",
    "        cv2.imshow(f\"Frame {i+1}\", frame)\n",
    "        cv2.waitKey(1000)  # Display each frame for 1 second\n",
    "        cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d876cce",
   "metadata": {},
   "source": [
    "## Function to display video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b723f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "def display_video(video_path):\n",
    "    return Video(video_path, width=640, height=480, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31610d7d",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ebb19ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  width=\"640\"  height=\"480\">\n",
       " <source src=\"data:None;base64,V_999.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = \"V_999.mp4\"\n",
    "#display_frames(video_path)\n",
    "display_video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d75d88",
   "metadata": {},
   "source": [
    "## Function to predict violence based on frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b9f1ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 103ms/step\n",
      "Prediction: Violent\n"
     ]
    }
   ],
   "source": [
    "def predict_violence(video_path):\n",
    "    frames = extract_frames(video_path)\n",
    "    X = np.array([frames])\n",
    "    prediction = model.predict(X)\n",
    "    if prediction > 0.5:\n",
    "        return \"Violent\"\n",
    "    else:\n",
    "        return \"Non-violent\"\n",
    "    \n",
    "video_path = os.path.join(violence_dir, \"V_999.mp4\")\n",
    "\n",
    "# Predict whether the video is violent or not\n",
    "prediction = predict_violence(video_path)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274c0c5",
   "metadata": {},
   "source": [
    "### Hence the Model is Classifying videos with an accuracy of around: 80%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
